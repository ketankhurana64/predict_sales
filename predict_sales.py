# -*- coding: utf-8 -*-
"""predict_sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oCdtB2rI8iUacPHiKfzM4cgWdC0KGKS
"""

from google.colab import files
files.upload()

!mv kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c competitive-data-science-predict-future-sales

!unzip items.csv.zip
!unzip test.csv.zip
!unzip sales_train.csv.zip

import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

pd.set_option('display.max_rows',100)
pd.set_option('display.max_columns',100)

df_sales=pd.read_csv('sales_train.csv')

df_items=pd.read_csv('items.csv')
df_itemcat=pd.read_csv('item_categories.csv')
df_shops=pd.read_csv('shops.csv')
df_test=pd.read_csv('test.csv')

df_sales.shape

def data_analysis(df):  
  print('########################################################################',end="\n")
  print('First 5 listings of dataframe',end="\n\n")
  print(df.head(5))    
  print('Dataframe info',end="\n\n")
  print(df.info())
  print('Describe dataframe',end="\n\n")
  print(df.describe())
  print('All Columns in the dataframe',end="\n\n")
  print(df.columns)
  print('Datatypes of different columns in dataframe',end="\n\n")
  print(df.dtypes)
  print('Count of Null values in the dataframe',end="\n\n")
  print(df.isnull().sum())
  print('Count of NA values in the dataframe',end="\n\n")
  print(df.isna().sum())
  print('Shape of dataframe',end="\n\n")
  print(df.shape)
  print("\n")

data_analysis(df_sales)

data_analysis(df_items)

data_analysis(df_itemcat)

data_analysis(df_shops)

"""Since 'date' column is date type -> convert it to datetime64"""

df_sales['date']=df_sales['date'].astype('datetime64')
print('Minimum data present:'+ str(df_sales["date"].min()))
print('Maximum date present:'+ str(df_sales["date"].max()))

df_sales['day']=df_sales['date'].dt.day
df_sales['month']=df_sales['date'].dt.month
df_sales['year']=df_sales['date'].dt.year
df_sales.drop('date',axis=1,inplace=True)
df_sales.head()

print('Minimum value:   '+ str(df_sales["item_cnt_day"].min()))
print('Maximum value:   '+ str(df_sales["item_cnt_day"].max()))

df_sales[df_sales['item_cnt_day'] < 0]

df_sales['item_cnt_day']=df_sales['item_cnt_day'].map(lambda x:np.abs(x))
df_sales[df_sales['item_cnt_day'] < 0]

fig,axes = plt.subplots(1,2,figsize=(15,5))
sns.boxplot(df_sales['item_cnt_day'],ax=axes[0])
axes[0].set_title('Boxplot')
sns.distplot(df_sales['item_cnt_day'],ax=axes[1])
axes[1].set_title('Distribution')
plt.suptitle('No of units sold(Item Cnt day)',fontsize="20")
plt.show()

df_sales[df_sales['item_cnt_day']>=1000]
df_sales.drop([2326930,2909818],axis=0,inplace=True)
df_sales[df_sales['item_cnt_day']>=1000]

for i in range(0,101,10):
    print('percentile value: ' +str(i)+ '    item_cnt_day: '+str(np.percentile(df_sales["item_cnt_day"],i)))    
print('--'*25)
for i in range(91,100):
    print('percentile value: '+str(i)+ '    item_cnt_day: '+str(np.percentile(df_sales["item_cnt_day"],i)))    
print('--'*25)
for i in range(1,10):
    k = 99 + i/10 
    print('percentile value: '+str(k)+ '    item_cnt_day: '+str(np.percentile(df_sales["item_cnt_day"],k)))

fig,axes = plt.subplots(1,2,figsize=(15,5))
sns.boxplot(df_sales['item_price'],ax=axes[0])
axes[0].set_title('Boxplot')
sns.distplot(df_sales['item_price'],ax=axes[1])
axes[1].set_title('Distribution')
plt.suptitle('Item Price per unit',fontsize="20")
plt.show()

for i in range(0,101,10):
    print('percentile value: '+str(i)+ '   item_price:' +str(np.percentile(df_sales["item_price"],i)))    
print('--'*25)
for i in range(91,100):
    print('percentile value: '+str(i)+ '   item_price: '+str(np.percentile(df_sales["item_price"],i)))    
print('--'*25)
for i in range(1,10):
    k = 99 + i/10 
    print('percentile value: '+str(i)+ '   item_price: '+str(np.percentile(df_sales["item_price"],k)))

df_sales.drop(['date_block_num', 'item_price'], axis=1, inplace=True)
df_sales.head()

X=df_sales.drop('item_cnt_day',axis=1)
y=df_sales.loc[:,'item_cnt_day']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=42)

min_max_scaler = MinMaxScaler()
X_scaled = min_max_scaler.fit_transform(X_train)

from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
import pickle

def model_evaluation(modelname,model_):
  print("Training model :"+modelname+"\n\n")
  model=model_
  model.fit(X_scaled,y_train)
  predicted=model.predict(X_test)
  print("Mean squared error for model "+modelname+ " is:" +str(sqrt(mean_squared_error(y_test, predicted))))
  print("Mean absolute error model "+modelname+" is:   "+str(mean_absolute_error(y_test,predicted)))
  print("Model score for model "+modelname+" is : "+str(model.score(X_train,y_train)))   
  print(model.get_params())
  
  pickle.dump(model,open('model_'+modelname+'.pkl','wb'))
#   files.download('model_'+modelname+'.pkl')

models={'linearregression':LinearRegression(),'decisiontree':DecisionTreeRegressor(),'adaboost':AdaBoostRegressor(),'randomforest':RandomForestRegressor()}
for model_name,model in models.items():
  model_evaluation(model_name,model)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score

random_search = {
               'max_depth': [5,10,20],               
               'min_samples_leaf': [4, 12],
               'max_depth': [int(val) for val in np.linspace(10, 40, num = 3,dtype=int)],
               'min_samples_split': [5, 10, 15],
               'n_estimators': [int(val) for val in np.linspace(start = 200, stop = 2000, num = 4,dtype=int)],
               }

model = RandomizedSearchCV(estimator = RandomForestRegressor(), param_distributions = random_search, 
                               cv = 2, verbose= 5, random_state= 42, n_jobs = -1)
model_evaluation('hyperparametrised_rf',model)

